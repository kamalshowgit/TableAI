---
title: 'API Reference'
description: 'Complete API documentation for TableAI functions and classes'
---

## Overview

TableAI provides a comprehensive set of functions for data processing, AI integration, and secure code execution. This reference documents all public functions, their parameters, return values, and usage examples.

## Core Components

### Application Structure

```python
# Main application entry point
app.py                    # Streamlit application with AI integration
â”œâ”€â”€ Data Handling         # File upload and database connections
â”œâ”€â”€ AI Integration        # LlamaIndex + Ollama integration
â”œâ”€â”€ Code Execution        # Safe execution environment
â”œâ”€â”€ UI Components         # Custom Streamlit interface
â””â”€â”€ Security Layer        # Code sanitization and validation
```

## Function Categories

<CardGroup cols={2}>
  <Card
    title="Data Processing"
    icon="database"
    href="#data-processing"
  >
    Functions for file handling, data loading, and format conversion
  </Card>
  <Card
    title="AI Integration"
    icon="robot"
    href="#ai-integration"
  >
    LLM and embedding model management with caching
  </Card>
  <Card
    title="Security & Safety"
    icon="shield-check"
    href="#security-functions"
  >
    Code sanitization and safe execution utilities
  </Card>
  <Card
    title="UI Components"
    icon="window"
    href="#ui-components"
  >
    Custom Streamlit interface components and rendering
  </Card>
</CardGroup>

---

## Data Processing

### `save_and_get_temp_path(uploaded_file)`

Saves an uploaded file to a temporary location and returns the path.

**Parameters:**
- `uploaded_file` (streamlit.UploadedFile): File uploaded through Streamlit interface

**Returns:**
- `str`: Absolute path to the temporary file

**Example:**
```python
uploaded_file = st.file_uploader("Choose a file")
if uploaded_file:
    temp_path = save_and_get_temp_path(uploaded_file)
    print(f"File saved to: {temp_path}")
```

**Implementation Details:**
- Generates unique filename using UUID
- Preserves original file extension
- Uses system temporary directory
- Logs file operations for audit trail

---

### `load_dataframe_from_temp(temp_path)`

Loads a pandas DataFrame from a temporary file path, supporting multiple formats.

**Parameters:**
- `temp_path` (str): Absolute path to the temporary file

**Returns:**
- `pandas.DataFrame`: Loaded data or empty DataFrame on error

**Supported Formats:**
- CSV (`.csv`)
- TSV (`.tsv`) 
- Excel (`.xlsx`, `.xls`)
- Parquet (`.parquet`)
- JSON (`.json`)

**Example:**
```python
df = load_dataframe_from_temp("/tmp/data.csv")
if not df.empty:
    st.dataframe(df)
```

**Error Handling:**
- Returns empty DataFrame on failure
- Logs errors for debugging
- Shows user-friendly error messages

---

## AI Integration

### `get_llm()` ðŸš€

Returns a cached Ollama LLM instance for improved performance.

**Decorator:** `@st.cache_resource`

**Returns:**
- `Ollama`: Configured LLM instance with 120-second timeout

**Configuration:**
- Model: Configurable via `OLLAMA_MODEL` environment variable
- Default: `mistral`
- Timeout: 120 seconds
- Caching: Streamlit resource cache

**Example:**
```python
llm = get_llm()
response = llm.complete("Analyze this data...")
```

---

### `get_embed_model()` ðŸš€

Returns a cached Ollama embedding model instance.

**Decorator:** `@st.cache_resource`

**Returns:**
- `OllamaEmbedding`: Configured embedding model

**Configuration:**
- Model: Configurable via `OLLAMA_EMBED_MODEL` environment variable
- Default: `mistral`
- Caching: Streamlit resource cache

**Example:**
```python
embed_model = get_embed_model()
# Used internally for document indexing
```

---

## Security Functions

### `clean_ai_code(code)`

Post-processes AI-generated code to remove security risks and fix common errors.

**Parameters:**
- `code` (str): Raw AI-generated code string

**Returns:**
- `str`: Sanitized and cleaned code

**Security Measures:**
- Fixes matplotlib argument errors (`labels=` â†’ `label=`)
- Removes dangerous imports and function calls
- Blocks file system operations
- Prevents network access
- Removes code execution functions

**Blocked Patterns:**
```python
forbidden = [
    r'os\.',           # Operating system access
    r'subprocess',     # Process execution  
    r'open\(',         # File operations
    r'exec\(',         # Code execution
    r'eval\(',         # Dynamic evaluation
    r'import sys',     # System imports
    r'import socket',  # Network access
    r'requests',       # HTTP requests
    r'urllib'          # URL operations
]
```

**Example:**
```python
raw_code = "import os; os.system('rm -rf /')"
safe_code = clean_ai_code(raw_code)
# Result: "# BLOCKED; # BLOCKED('rm -rf /')"
```

---

## UI Components

### `render_analyst_output(ai_output)`

Renders different types of AI analysis outputs in the Streamlit interface.

**Parameters:**
- `ai_output` (Any): Output from AI analysis (DataFrame, plots, strings, etc.)

**Returns:**
- None (renders directly to Streamlit)

**Supported Output Types:**

<AccordionGroup>
  <Accordion title="pandas.DataFrame">
    - Displays as interactive Streamlit dataframe
    - Limits to 1000 rows for performance
    - Full-width container display
  </Accordion>
  
  <Accordion title="Matplotlib Figures">
    - Renders using `st.pyplot()`
    - Fallback to Plotly if available
    - Error handling with user-friendly messages
  </Accordion>
  
  <Accordion title="String Output">
    - Terminal-style formatting for code output
    - Special handling for file read messages
    - Styled with custom CSS classes
  </Accordion>
  
  <Accordion title="Plotly Charts">
    - Uses `st.plotly_chart()` when available
    - Graceful fallback when Plotly not installed
    - Interactive charts with full features
  </Accordion>
</AccordionGroup>

**Example:**
```python
# DataFrame output
df_result = pd.DataFrame({'A': [1, 2, 3]})
render_analyst_output(df_result)

# String output  
text_result = "Analysis complete: 3 rows processed"
render_analyst_output(text_result)

# Plot output
fig, ax = plt.subplots()
ax.plot([1, 2, 3], [1, 4, 2])
render_analyst_output(fig)
```

---

## Environment Configuration

### Environment Variables

TableAI uses environment variables for configuration:

```bash
# .env file
OLLAMA_MODEL=mistral              # Primary LLM model
OLLAMA_EMBED_MODEL=mistral        # Embedding model
DATABASE_URL=sqlite:///data.db    # Default database connection
LOG_LEVEL=INFO                    # Logging level
```

### Model Configuration

**Supported Ollama Models:**
- `mistral` (default) - Balanced performance and accuracy
- `llama2` - Alternative LLM option
- `codellama` - Specialized for code generation
- `neural-chat` - Conversational AI model

### Logging Configuration

```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(name)s %(message)s',
    handlers=[
        logging.FileHandler('tableai_app.log'),  # File logging
        logging.StreamHandler()                   # Console logging
    ]
)
```

---

## Error Handling

### Common Error Patterns

<CodeGroup>

```python File Loading Errors
try:
    df = load_dataframe_from_temp(temp_path)
    if df.empty:
        st.error("No data loaded. Check file format.")
except Exception as e:
    logger.error(f"Error loading file: {e}")
    st.error(f"Error loading file: {e}")
```

```python AI Processing Errors
try:
    llm = get_llm()
    response = llm.complete(question)
except Exception as e:
    st.session_state['analyst_last_output'] = f"Error: {e}"
    logger.error(f"AI processing failed: {e}")
```

```python Code Execution Errors
try:
    exec(cleaned_code, {}, local_vars)
    result = local_vars.get('result')
except ValueError as ve:
    if "Length mismatch" in str(ve):
        # Handle DataFrame column mismatch
    elif "got an unexpected keyword argument" in str(ve):
        # Handle matplotlib argument errors
except TypeError as te:
    # Handle aggregation on non-numeric columns
```

</CodeGroup>

### Error Recovery Strategies

1. **Graceful Degradation**: Show partial results when possible
2. **User Feedback**: Clear error messages with suggested actions
3. **Logging**: Comprehensive error logging for debugging
4. **Fallbacks**: Alternative processing methods when primary fails

---

## Performance Optimization

### Caching Strategy

**Model Caching:**
```python
@st.cache_resource
def get_llm():
    return Ollama(model=OLLAMA_MODEL, request_timeout=120)
```

**Data Caching:**
- Session state for user data
- Temporary file management
- Efficient DataFrame operations

### Memory Management

- Automatic cleanup of temporary files
- Garbage collection for large datasets
- Chunked processing for memory efficiency

### Response Time Optimization

- Cached model loading (one-time initialization)
- Optimized data processing pipelines
- Async operations where possible

---

## Security Architecture

### Multi-Layer Security

<Steps>
  <Step title="Input Validation">
    All user inputs are validated and sanitized before processing
  </Step>
  <Step title="Code Sanitization">
    AI-generated code is cleaned and filtered for security risks
  </Step>
  <Step title="Sandboxed Execution">
    Code runs in controlled environment with limited scope
  </Step>
  <Step title="Local Processing">
    All operations happen locally - no external API calls
  </Step>
</Steps>

### Security Functions Reference

- `clean_ai_code()` - Primary code sanitization
- Error boundary handling - Prevents crashes and data exposure
- Session isolation - User data separation
- Audit logging - Security event tracking
