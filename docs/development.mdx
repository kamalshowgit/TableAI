---
title: 'Development'
description: 'Set up TableAI for development and contribution'
---

## Development Setup

### Prerequisites

- Python 3.8 or higher
- Node.js 16+ (for documentation)
- Git
- Ollama installed locally

### Local Development

<Steps>
  <Step title="Clone Repository">
    ```bash
    git clone https://github.com/kamalshowgit/TableAI.git
    cd TableAI
    ```
  </Step>
  
  <Step title="Create Virtual Environment">
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```
  </Step>
  
  <Step title="Install Dependencies">
    ```bash
    pip install -r requirements.txt
    ```
  </Step>
  
  <Step title="Set Up Environment">
    ```bash
    cp .env.example .env
    # Edit .env with your preferred settings
    ```
  </Step>
  
  <Step title="Start Development Server">
    ```bash
    streamlit run app.py
    ```
  </Step>
</Steps>

## Project Structure

```
TableAI/
├── app.py                 # Main Streamlit application
├── requirements.txt       # Python dependencies
├── runtime.txt           # Python runtime version
├── .env.example          # Environment variables template
├── PRODUCTION.md         # Production deployment guide
├── README.md             # Project documentation
├── sample_data.sql       # Sample database data
├── sql_db.py             # Database utilities
├── tableai_app.log       # Application logs
├── screenshots/          # Application screenshots
└── docs/                 # Documentation (Mintlify)
    ├── mint.json         # Mintlify configuration
    ├── introduction.mdx  # Introduction page
    └── ...               # Other documentation pages
```

## Key Components

### Core Application (`app.py`)

The main application file contains:

- **UI Components**: Streamlit interface with custom CSS
- **Data Handling**: File upload and database connection logic
- **AI Integration**: LlamaIndex and Ollama integration
- **Code Execution**: Safe execution environment for generated code
- **Caching**: Model and embedding caching for performance

### Key Functions

<CodeGroup>

```python clean_ai_code()
def clean_ai_code(code: str) -> str:
    """
    Post-process AI-generated code to fix common mistakes and improve safety.
    """
    # Fix matplotlib argument errors
    code = re.sub(r'labels\s*=', 'label=', code)
    
    # Remove dangerous patterns
    forbidden = [r'os\.', r'subprocess', r'open\(', ...]
    for pattern in forbidden:
        code = re.sub(pattern, '# BLOCKED', code)
    
    return code.strip()
```

```python get_llm() & get_embed_model()
@st.cache_resource
def get_llm():
    """Get cached LLM instance for better performance."""
    return Ollama(model=OLLAMA_MODEL, request_timeout=120)

@st.cache_resource 
def get_embed_model():
    """Get cached embedding model instance for better performance."""
    return OllamaEmbedding(model_name=OLLAMA_EMBED_MODEL)
```

```python render_analyst_output()
def render_analyst_output(ai_output: Any) -> None:
    """Render different types of AI outputs appropriately."""
    if isinstance(ai_output, pd.DataFrame):
        st.dataframe(ai_output, use_container_width=True)
    elif hasattr(ai_output, 'figure'):
        st.pyplot(ai_output)
    # ... handle other output types
```

</CodeGroup>

## Security Features

TableAI implements several security measures:

<AccordionGroup>
  <Accordion title="Code Sanitization">
    - Removes dangerous imports and function calls
    - Blocks file system operations
    - Prevents network access from generated code
  </Accordion>
  
  <Accordion title="Local Processing">
    - All AI processing happens locally via Ollama
    - No data sent to external APIs
    - User data stays on their machine
  </Accordion>
  
  <Accordion title="Sandboxed Execution">
    - Generated code runs in controlled environment
    - Limited variable scope
    - Error handling and timeouts
  </Accordion>
</AccordionGroup>

## Contributing

### Code Style

- Follow PEP 8 for Python code
- Use type hints where appropriate
- Add docstrings for functions and classes
- Keep functions focused and small

### Testing

Run tests before submitting:

```bash
# Install test dependencies
pip install pytest streamlit-testing

# Run tests
pytest tests/
```

### Pull Requests

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes with tests
4. Commit with clear messages
5. Push and create a pull request

## Documentation Development

This documentation is built with Mintlify. To develop locally:

```bash
# Install Mintlify CLI
npm install -g mintlify

# Navigate to docs
cd docs

# Start development server
mintlify dev
```

## Deployment

See [PRODUCTION.md](../PRODUCTION.md) for detailed deployment instructions including:

- Environment setup
- Database configuration
- Performance optimization
- Monitoring and logging

## Troubleshooting Development Issues

<Tip>
**Model Loading Issues**: Ensure Ollama is running and models are pulled: `ollama list`
</Tip>

<Tip>
**Memory Issues**: Large datasets may require increasing Python memory limits or processing in chunks
</Tip>

<Warning>
**Port Conflicts**: Default Streamlit port is 8501. Use `--server.port` to change if needed
</Warning>
